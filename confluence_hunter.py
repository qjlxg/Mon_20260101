import pandas as pd
import os
import glob
from datetime import datetime

# --- é…ç½®åŒº ---
RESULTS_DIR = 'results'
REPORT_DIR = 'reports'

# 16ä¸ªæˆ˜æ³•ç›®å½•ä¸ä¸­æ–‡åç§°çš„å¯¹åº”å…³ç³»
STRATEGY_MAP = {
    'macd_bottom': 'MACDæŠ„åº•',
    'duck_head': 'è€é¸­å¤´',
    'three_in_one': 'ä¸‰ä½ä¸€ä½“',
    'pregnancy_line': 'åº•éƒ¨å­•çº¿',
    'single_yang': 'å•é˜³ä¸ç ´',
    'limit_pullback': 'æ¶¨åœå›è°ƒ',
    'golden_pit': 'é»„é‡‘å‘',
    'grass_fly': 'è‰ä¸Šé£',
    'limit_break': 'æ¶¨åœç ´ä½',
    'double_plate': 'é˜´é˜³åŒæ¿',
    'horse_back': 'æ´—ç›˜å›é©¬æª',
    'hot_money': 'æ¸¸èµ„å›è°ƒ',
    'wave_bottom': 'æ³¢åŠ¨æŠ„åº•',
    'no_loss': 'ç‰›æ•£ä¸äºé’±',
    'chase_rise': 'é«˜æ‰‹è¿½æ¶¨',
    'inst_swing': 'æœºæ„æ³¢æ®µ'
}

def run_confluence_hunter():
    # è·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸²
    date_str = datetime.now().strftime('%Y-%m-%d')
    all_data = []

    print(f"å¼€å§‹æ±‡æ€» {date_str} çš„ 16 æˆ˜æ³•ç­›é€‰ç»“æœ...")

    # 1. éå† 16 ä¸ªæˆ˜æ³•æ–‡ä»¶å¤¹
    for s_key, s_name in STRATEGY_MAP.items():
        folder_path = os.path.join(RESULTS_DIR, s_key)
        if not os.path.exists(folder_path):
            continue
            
        # æŸ¥æ‰¾å½“å¤©çš„ç»“æœæ–‡ä»¶ï¼Œä¾‹å¦‚ï¼šresults/macd_bottom/macd_bottom_2025-12-29.csv
        file_pattern = os.path.join(folder_path, f"{s_key}_{date_str}.csv")
        target_files = glob.glob(file_pattern)
        
        for f in target_files:
            try:
                df = pd.read_csv(f, dtype={'code': str})
                if df.empty:
                    continue
                # æ ‡è®°è¯¥è‚¡ç¥¨æ‰€å±çš„æˆ˜æ³•åç§°
                df['match_strategy'] = s_name
                all_data.append(df)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {f} å‡ºé”™: {e}")

    # 2. å¦‚æœæ²¡æœ‰é€‰å‡ºä»»ä½•è‚¡ç¥¨ï¼Œç›´æ¥é€€å‡º
    if not all_data:
        print(f"ä»Šæ—¥ ({date_str}) 16 ä¸ªæˆ˜æ³•å‡æœªé€‰å‡ºç›®æ ‡ã€‚")
        return

    # 3. åˆå¹¶æ‰€æœ‰æˆ˜æ³•é€‰å‡ºçš„è‚¡ç¥¨
    full_df = pd.concat(all_data, ignore_index=True)

    # 4. æ ¸å¿ƒé€»è¾‘ï¼šç»Ÿè®¡æ¯åªè‚¡ç¥¨å‡ºç°çš„æ¬¡æ•°ï¼ˆå…±æŒ¯å¼ºåº¦ï¼‰
    # åˆ†ç»„ç»Ÿè®¡ï¼šä»£ç ã€åç§°ã€ä»·æ ¼
    summary = full_df.groupby(['code', 'name']).agg({
        'match_strategy': lambda x: ' + '.join(list(x)), # åˆå¹¶æ‰€æœ‰å‘½ä¸­çš„æˆ˜æ³•å
        'price': 'last' # è®°å½•ä»·æ ¼
    }).reset_index()

    # è®¡ç®—å…±æŒ¯æ¬¡æ•°
    summary['count'] = summary['match_strategy'].apply(lambda x: len(x.split(' + ')))
    
    # æŒ‰å…±æŒ¯å¼ºåº¦ä»é«˜åˆ°ä½æ’åº
    summary = summary.sort_values(by='count', ascending=False)
    
    # é‡å‘½ååˆ—åä»¥ä¾¿é˜…è¯»
    summary.columns = ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'å‘½ä¸­æˆ˜æ³•æ±‡æ€»', 'æœ€æ–°ä»·', 'å…±æŒ¯å¼ºåº¦']

    # 5. ä¿å­˜æ±‡æ€»ç»“æœ
    if not os.path.exists(REPORT_DIR):
        os.makedirs(REPORT_DIR)

    # ä¿å­˜ä¸º CSV æ–¹ä¾¿ä¸‹è½½
    csv_output = os.path.join(REPORT_DIR, f"confluence_{date_str}.csv")
    summary.to_csv(csv_output, index=False, encoding='utf-8-sig')

    # 6. ç”Ÿæˆ Markdown å¤ç›˜æŠ¥å‘Š
    md_output = os.path.join(REPORT_DIR, f"report_{date_str}.md")
    with open(md_output, 'w', encoding='utf-8') as f:
        f.write(f"# 16 æˆ˜æ³•å…±æŒ¯å¤ç›˜æŠ¥å‘Š ({date_str})\n\n")
        f.write(f"ä»Šæ—¥å…¨å¸‚åœºæ‰«æå®Œæˆã€‚å…±æœ‰ **{len(summary)}** åªè‚¡ç¥¨å…¥é€‰æˆ˜æ³•æ± ã€‚\n\n")
        
        # æå–å¤šé‡å…±æŒ¯çš„è‚¡ç¥¨ï¼ˆå…±æŒ¯å¼ºåº¦ >= 2ï¼‰
        strong_signals = summary[summary['å…±æŒ¯å¼ºåº¦'] >= 2]
        
        f.write("## ğŸš€ å¼ºå…±æŒ¯æé†’ (å¤šé‡æˆ˜æ³•æŒ‡å‘)\n")
        if not strong_signals.empty:
            f.write(strong_signals.to_markdown(index=False))
        else:
            f.write("ä»Šæ—¥æš‚æ— åŒé‡åŠä»¥ä¸Šå…±æŒ¯çš„æ ‡çš„ã€‚\n")
            
        f.write("\n\n## ğŸ“‹ å…¨é‡å…¥é€‰æ¸…å• (æŒ‰å¼ºåº¦æ’åº)\n")
        f.write(summary.to_markdown(index=False))

    print(f"âœ… æ±‡æ€»å®Œæˆï¼å…±æŒ¯æŠ¥å‘Šå·²ç”Ÿæˆè‡³: {md_output}")

if __name__ == "__main__":
    run_confluence_hunter()
